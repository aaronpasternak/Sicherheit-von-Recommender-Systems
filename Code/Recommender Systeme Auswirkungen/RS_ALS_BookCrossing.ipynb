{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a6192-ff98-4394-b34e-8e3a103b2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der erforderlichen Bibliotheken\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, expr, collect_list, size, array_intersect, percent_rank, abs\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Spark Session initialisieren\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BookCrossing_ALS\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722b5ce-6027-48d4-8de3-e55976dcb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Ratings-Datensatzes (mit delimiter \";\")\n",
    "ratings1 = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"BX-Book-Ratings.csv\")\n",
    "\n",
    "# Skaliere die Book-Rating-Spalte auf eine 5er-Skala\n",
    "ratings1 = ratings1.withColumn(\"Book-Rating\", col(\"Book-Rating\") / 2.0)\n",
    "\n",
    "# Entferne fehlende Werte und Duplikate\n",
    "ratings1 = ratings1.dropna().dropDuplicates()\n",
    "\n",
    "# Damit die Nutzer-IDs aus beiden Datensätzen zusammengeführt werden können,\n",
    "# benenne die ursprüngliche Nutzer-Spalte in Datensatz 1 um\n",
    "ratings1 = ratings1.withColumnRenamed(\"User-ID\", \"userId_orig\")\n",
    "# Zeige 20 Einträge aus Datensatz 1\n",
    "ratings1.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ce1db-60cd-41c9-9289-587b6f75b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Datensatzes nach Angriff (tausche Angriffsart und Angriffsgröße jeweils aus)\n",
    "ratings2 = spark.read.option(\"header\", \"true\").csv(\"all_ratings_BookCrossing_reverse_bandwagon_20.0.csv\")\n",
    "# Entferne fehlende Werte und Duplikate\n",
    "ratings2 = ratings2.dropna().dropDuplicates()\n",
    "\n",
    "# Skaliere die Rating-Spalte auf eine 5er-Skala\n",
    "ratings2 = ratings2.withColumn(\"rating\", col(\"rating\") / 2.0)\n",
    "\n",
    "# Benenne die Nutzer-Spalte um, damit beide Datensätze denselben Namen haben\n",
    "ratings2 = ratings2.withColumnRenamed(\"userId\", \"userId_orig\")\n",
    "# Zeige 20 Einträge aus Datensatz 2\n",
    "ratings2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a37513-4eeb-4d68-9e0d-f438a34902e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombiniere alle eindeutigen Nutzer-IDs aus beiden Datensätzen\n",
    "combined_users = ratings1.select(\"userId_orig\").union(ratings2.select(\"userId_orig\")).distinct()\n",
    "\n",
    "# Fitte einen StringIndexer für die Nutzer auf dem kombinierten Datensatz\n",
    "user_indexer = StringIndexer(inputCol=\"userId_orig\", outputCol=\"userId\", handleInvalid=\"skip\")\n",
    "user_indexer_model = user_indexer.fit(combined_users)\n",
    "\n",
    "# Transformiere beide Datensätze mit dem gleichen Nutzer-Indexer\n",
    "ratings1 = user_indexer_model.transform(ratings1)\n",
    "ratings2 = user_indexer_model.transform(ratings2)\n",
    "\n",
    "# Für Items: Beide Datensätze besitzen die Spalte \"ISBN\", also:\n",
    "combined_items = ratings1.select(\"ISBN\").union(ratings2.select(\"ISBN\")).distinct()\n",
    "item_indexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"ISBN_new\", handleInvalid=\"skip\")\n",
    "item_indexer_model = item_indexer.fit(combined_items)\n",
    "\n",
    "# Wende den Item-Indexer auf beide Datensätze an\n",
    "ratings1 = item_indexer_model.transform(ratings1)\n",
    "ratings2 = item_indexer_model.transform(ratings2)\n",
    "\n",
    "# Für Datensatz 1: Indexierung der Ratings (Spalte \"Book-Rating\"); Ergebnis in \"rating\"\n",
    "rating_indexer1 = StringIndexer(inputCol=\"Book-Rating\", outputCol=\"rating\", handleInvalid=\"skip\")\n",
    "ratings1 = rating_indexer1.fit(ratings1).transform(ratings1)\n",
    "\n",
    "# Für Datensatz 2: Indexierung der Ratings (Spalte \"rating\"); Ergebnis in \"rating_new\"\n",
    "rating_indexer2 = StringIndexer(inputCol=\"rating\", outputCol=\"rating_new\", handleInvalid=\"skip\")\n",
    "ratings2 = rating_indexer2.fit(ratings2).transform(ratings2)\n",
    "\n",
    "# Entferne in Datensatz 1 unnötige Spalten\n",
    "ratings1 = ratings1.drop('userId_orig', 'Book-Rating', 'ISBN')\n",
    "ratings1.printSchema()\n",
    "\n",
    "# Entferne in Datensatz 2 die ursprünglichen Spalten (z. B. \"userId_orig\", \"rating\", \"Label\", \"ISBN\")\n",
    "ratings2 = ratings2.drop('userId_orig', 'rating', 'Label', 'ISBN')\n",
    "ratings2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9daab5-c91c-43c8-803e-2be203cc2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teile die Daten in Trainings- und Testdaten auf in einem 80/20-Verhältnis\n",
    "train1, test1 = ratings1.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Datensatz 1: Trainingsset: {train1.count()} Einträge, Testset: {test1.count()} Einträge\")\n",
    "\n",
    "train2, test2 = ratings2.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Datensatz 2: Trainingsset: {train2.count()} Einträge, Testset: {test2.count()} Einträge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c3d82-f085-4dc9-9257-651da99c45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS-Algorithmus für Datensatz 1 (vor Angriff)\n",
    "als1 = ALS(\n",
    "    rank=100,\n",
    "    maxIter=20,\n",
    "    regParam=0.05,\n",
    "    userCol='userId',\n",
    "    itemCol='ISBN_new',\n",
    "    ratingCol='rating',\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "als_model1 = als1.fit(train1)\n",
    "predictions1 = als_model1.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e66ad-da15-492e-8628-8e599efd91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE und MAE für Datensatz 1 berechnen\n",
    "evaluator_rmse1 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_mae1 = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse1 = evaluator_rmse1.evaluate(predictions1)\n",
    "mae1 = evaluator_mae1.evaluate(predictions1)\n",
    "\n",
    "print(f\"Datensatz 1 - RMSE: {rmse1}\")\n",
    "print(f\"Datensatz 1 - MAE: {mae1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae31c09-9ee2-4aea-85dc-4374eb5a3c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HitRate-Berechnung für Datensatz 1\n",
    "window_spec1 = Window.orderBy(col(\"rating\").desc())\n",
    "test1 = test1.withColumn(\"percentile\", percent_rank().over(window_spec1))\n",
    "# Verwende die besten 20% der Bewertungen als relevante Items und gruppiere nach userId\n",
    "relevant_items1 = test1.filter(col(\"percentile\") <= 0.2) \\\n",
    "                       .groupBy(\"userId\") \\\n",
    "                       .agg(expr(\"collect_list(ISBN_new) as relevant_items\"))\n",
    "\n",
    "# Verwende 10 und 20 als Werte für k, um die HitRate@10 sowie HitRate@20 zu berechnen\n",
    "k_values = [10, 20]\n",
    "for k in k_values:\n",
    "    top_k_recommendations1 = als_model1.recommendForAllUsers(k)\n",
    "    top_k_recommendations_exploded1 = top_k_recommendations1.withColumn(\n",
    "        \"recommended_item_ids\",\n",
    "        expr(\"transform(recommendations, x -> x['ISBN_new'])\")\n",
    "    )\n",
    "    joined_data1 = top_k_recommendations_exploded1.join(relevant_items1, on=\"userId\", how=\"inner\")\n",
    "    hit_data1 = joined_data1.withColumn(\n",
    "        \"relevant_in_recommendations\",\n",
    "        size(array_intersect(col(\"recommended_item_ids\"), col(\"relevant_items\")))\n",
    "    )\n",
    "    hit_data1 = hit_data1.withColumn(\"is_hit\", col(\"relevant_in_recommendations\") > 0)\n",
    "    count_joined1 = joined_data1.count()\n",
    "# Berechne die durchschnittliche HitRate über alle User\n",
    "    hit_rate1 = hit_data1.filter(col(\"is_hit\") == True).count() / count_joined1 if count_joined1 > 0 else None\n",
    "    print(f\"Datensatz 1 - HitRate@{k}: {hit_rate1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b60b7-6be6-4dce-8bb4-1dc567535163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS-Algorithmus für Datensatz 2 (nach Angriff)\n",
    "als2 = ALS(\n",
    "    rank=100,\n",
    "    maxIter=20,\n",
    "    regParam=0.05,\n",
    "    userCol='userId',\n",
    "    itemCol='ISBN_new',\n",
    "    ratingCol='rating_new',\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "als_model2 = als2.fit(train2)\n",
    "predictions2 = als_model2.transform(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd6830-9a8e-45fa-9a1f-d80b89098ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE und MAE für Datensatz 2 berechnen\n",
    "evaluator_rmse2 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating_new\", predictionCol=\"prediction\")\n",
    "evaluator_mae2 = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating_new\", predictionCol=\"prediction\")\n",
    "rmse2 = evaluator_rmse2.evaluate(predictions2)\n",
    "mae2 = evaluator_mae2.evaluate(predictions2)\n",
    "\n",
    "print(f\"Datensatz 2 - RMSE: {rmse2}\")\n",
    "print(f\"Datensatz 2 - MAE: {mae2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a34106-781a-4798-ae50-66a3c1fc297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HitRate-Berechnung für Datensatz 2\n",
    "window_spec2 = Window.orderBy(col(\"rating_new\").desc())\n",
    "test2 = test2.withColumn(\"percentile\", percent_rank().over(window_spec2))\n",
    "# Verwende die besten 20% der Bewertungen als relevante Items und gruppiere nach userId\n",
    "relevant_items2 = test2.filter(col(\"percentile\") <= 0.2) \\\n",
    "                       .groupBy(\"userId\") \\\n",
    "                       .agg(expr(\"collect_list(ISBN_new) as relevant_items\"))\n",
    "\n",
    "for k in k_values:\n",
    "    top_k_recommendations2 = als_model2.recommendForAllUsers(k)\n",
    "    top_k_recommendations_exploded2 = top_k_recommendations2.withColumn(\n",
    "        \"recommended_item_ids\",\n",
    "        expr(\"transform(recommendations, x -> x['ISBN_new'])\")\n",
    "    )\n",
    "    joined_data2 = top_k_recommendations_exploded2.join(relevant_items2, on=\"userId\", how=\"inner\")\n",
    "    hit_data2 = joined_data2.withColumn(\n",
    "        \"relevant_in_recommendations\",\n",
    "        size(array_intersect(col(\"recommended_item_ids\"), col(\"relevant_items\")))\n",
    "    )\n",
    "    hit_data2 = hit_data2.withColumn(\"is_hit\", col(\"relevant_in_recommendations\") > 0)\n",
    "    count_joined2 = joined_data2.count()\n",
    "# Berechne die durchschnittliche HitRate über alle User\n",
    "    hit_rate2 = hit_data2.filter(col(\"is_hit\") == True).count() / count_joined2 if count_joined2 > 0 else None\n",
    "    print(f\"Datensatz 2 - HitRate@{k}: {hit_rate2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858c767-e432-4061-9d94-92d0d0a4309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne Vorhersagen auf den Trainingsdaten beider Modelle\n",
    "train_predictions1 = als_model1.transform(train1).select(\"userId\", \"ISBN_new\", col(\"prediction\").alias(\"prediction_1\"))\n",
    "train_predictions2 = als_model2.transform(train2).select(\"userId\", \"ISBN_new\", col(\"prediction\").alias(\"prediction_2\"))\n",
    "\n",
    "# Join der Vorhersagen über die gemeinsamen Schlüssel\n",
    "common_predictions = train_predictions1.join(train_predictions2, on=[\"userId\", \"ISBN_new\"], how=\"inner\")\n",
    "\n",
    "# Berechne den absoluten Unterschied und den durchschnittlichen Prediction Shift\n",
    "common_predictions = common_predictions.withColumn(\"abs_diff\", abs(col(\"prediction_1\") - col(\"prediction_2\")))\n",
    "prediction_shift = common_predictions.agg({\"abs_diff\": \"avg\"}).collect()[0][0]\n",
    "print(f\"Prediction Shift: {prediction_shift}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a2faf-602c-460b-a961-b7b2a412e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beende die Spark-Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
