{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ae0e1-96e4-4ae8-a819-aed9f9a8f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der erforderlichen Bibliotheken\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, size, array_intersect, percent_rank, abs, broadcast, slice\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0894a7-6770-49dd-a164-38f880a95820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session initialisieren\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Yelp_ALS\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2047m\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867aa68-7a5d-4583-b720-2958f7a2bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lese den Yelp JSON-Datensatz ein\n",
    "ratings1 = spark.read.json(\"yelp_academic_dataset_review.json\")\n",
    "ratings1 = ratings1.dropna().dropDuplicates()\n",
    "\n",
    "# Um eine gemeinsame Indexierung zu ermöglichen, benenne die Originalspalten um:\n",
    "# \"user_id\" → \"user_id_orig\" und \"business_id\" → \"business_id_orig\"\n",
    "ratings1 = ratings1.withColumnRenamed(\"user_id\", \"user_id_orig\") \\\n",
    "                   .withColumnRenamed(\"business_id\", \"business_id_orig\")\n",
    "# Die Bewertungs-Spalte \"stars\" bleibt unverändert.\n",
    "# Entferne nicht benötigte Spalten\n",
    "for c in [\"cool\", \"date\", \"funny\", \"review_id\", \"text\", \"useful\"]:\n",
    "    if c in ratings1.columns:\n",
    "        ratings1 = ratings1.drop(c)\n",
    "# Zeige 20 Einträge aus Datensatz 1\n",
    "ratings1.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf787f2f-0a6b-4ffb-8906-6de4e45aec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplikate entfernen\n",
    "ratings1 = ratings1.dropDuplicates([\"user_id_orig\", \"business_id_orig\"])\n",
    "print(ratings1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891faab-2791-461d-9b00-b35632aadf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Datensatzes nach Angriff (tausche Angriffsart und Angriffsgröße jeweils aus)\n",
    "ratings2 = spark.read.option(\"header\", \"true\").csv(\"all_ratings_Yelp_reverse_bandwagon_20.0.csv\")\n",
    "# Entferne fehlende Werte und Duplikate\n",
    "ratings2 = ratings2.dropna().dropDuplicates()\n",
    "\n",
    "# Um Spalten mit Datensatz 1 zusammenzuführen, benennen wir auch hier um:\n",
    "ratings2 = ratings2.withColumnRenamed(\"userId\", \"user_id_orig\") \\\n",
    "                   .withColumnRenamed(\"businessId\", \"business_id_orig\")\n",
    "# Stelle sicher, dass \"stars\" als Float vorliegt:\n",
    "ratings2 = ratings2.withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "# Entferne nicht benötigte Spalten\n",
    "ratings2 = ratings2.drop(\"Label\")\n",
    "# Zeige 20 Einträge aus Datensatz 2\n",
    "ratings2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e3209-e515-4b87-95d6-f509963f4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombiniere alle eindeutigen Nutzer-IDs aus beiden Datensätzen\n",
    "combined_users = ratings1.select(\"user_id_orig\").union(ratings2.select(\"user_id_orig\")).distinct()\n",
    "user_indexer = StringIndexer(inputCol=\"user_id_orig\", outputCol=\"userId\", handleInvalid=\"skip\")\n",
    "user_indexer_model = user_indexer.fit(combined_users)\n",
    "\n",
    "# Wende den gleichen Nutzer-Indexer auf beide Datensätze an\n",
    "ratings1 = user_indexer_model.transform(ratings1)\n",
    "ratings2 = user_indexer_model.transform(ratings2)\n",
    "\n",
    "# Items: Vereine alle eindeutigen business_id_orig aus beiden Datensätzen\n",
    "combined_businesses = ratings1.select(\"business_id_orig\").union(ratings2.select(\"business_id_orig\")).distinct()\n",
    "business_indexer = StringIndexer(inputCol=\"business_id_orig\", outputCol=\"businessId_new\", handleInvalid=\"skip\")\n",
    "business_indexer_model = business_indexer.fit(combined_businesses)\n",
    "\n",
    "# Wende den Item-Indexer auf beide Datensätze an\n",
    "ratings1 = business_indexer_model.transform(ratings1)\n",
    "ratings2 = business_indexer_model.transform(ratings2)\n",
    "\n",
    "# Entferne die Originalspalten, da ausschließlich mit den indexierten Spalten weitergearbeitet wird\n",
    "ratings1 = ratings1.drop(\"user_id_orig\", \"business_id_orig\")\n",
    "ratings2 = ratings2.drop(\"user_id_orig\", \"business_id_orig\")\n",
    "ratings2 = ratings2.withColumnRenamed(\"rating\", \"stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd254d4-6a7d-46aa-b9e0-c8c0115c6d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Teile die Daten in Trainings- und Testdaten auf in einem 80/20-Verhältnis\n",
    "train1, test1 = ratings1.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Datensatz 1: Trainingsset: {train1.count()} Einträge, Testset: {test1.count()} Einträge\")\n",
    "\n",
    "train2, test2 = ratings2.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Datensatz 2: Trainingsset: {train2.count()} Einträge, Testset: {test2.count()} Einträge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96e633-b902-4433-aae2-b61e20a35192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS-Algorithmus für Datensatz 1 (vor Angriff)\n",
    "als1 = ALS(\n",
    "    rank=100,\n",
    "    maxIter=20,\n",
    "    regParam=0.05,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"businessId_new\",\n",
    "    ratingCol=\"stars\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "als_model1 = als1.fit(train1)\n",
    "predictions1 = als_model1.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e565e-e379-4efe-84f1-52a7eab79704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RMSE und MAE für Datensatz 1 berechnen\n",
    "evaluator_rmse1 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"stars\", predictionCol=\"prediction\")\n",
    "evaluator_mae1 = RegressionEvaluator(metricName=\"mae\", labelCol=\"stars\", predictionCol=\"prediction\")\n",
    "rmse1 = evaluator_rmse1.evaluate(predictions1)\n",
    "mae1 = evaluator_mae1.evaluate(predictions1)\n",
    "\n",
    "print(f\"Datensatz 1 - RMSE: {rmse1}\")\n",
    "print(f\"Datensatz 1 - MAE: {mae1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df8770-f47f-404f-a50e-6d00cbd64d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HitRate-Berechnung für Datensatz 1\n",
    "\n",
    "# Anzahl der Partitionen optimieren\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "# Relevante Items nur einmal berechnen\n",
    "window_spec1 = Window.orderBy(col(\"stars\").desc())\n",
    "\n",
    "test1 = test1.withColumn(\"percentile\", percent_rank().over(window_spec1))\n",
    "# Verwende die besten 20% der Bewertungen als relevante Items und gruppiere nach userId\n",
    "relevant_items1 = test1.filter(col(\"percentile\") <= 0.2) \\\n",
    "                       .groupBy(\"userId\") \\\n",
    "                       .agg(expr(\"collect_list(businessId_new) as relevant_items\")) \\\n",
    "                       .cache()\n",
    "\n",
    "# Empfehlungen nur einmal berechnen\n",
    "top_k_recommendations = als_model1.recommendForAllUsers(20).cache()\n",
    "\n",
    "# Extrahiere empfohlene Items\n",
    "top_k_recommendations = top_k_recommendations.withColumn(\n",
    "    \"recommended_item_ids\",\n",
    "    expr(\"transform(recommendations, x -> x['businessId_new'])\")\n",
    ").select(\"userId\", \"recommended_item_ids\")\n",
    "\n",
    "# Verbinde mit relevanten Items (Broadcast für kleinere Tabellen)\n",
    "joined_data = top_k_recommendations.join(broadcast(relevant_items1), on=\"userId\", how=\"inner\")\n",
    "\n",
    "# Berechne Treffer für k=10 und k=20 gleichzeitig\n",
    "hit_data = joined_data.withColumn(\n",
    "    \"hits_10\",\n",
    "    size(array_intersect(slice(col(\"recommended_item_ids\"), 1, 10), col(\"relevant_items\")))\n",
    ").withColumn(\n",
    "    \"hits_20\",\n",
    "    size(array_intersect(col(\"recommended_item_ids\"), col(\"relevant_items\")))\n",
    ").withColumn(\n",
    "    \"is_hit_10\", col(\"hits_10\") > 0\n",
    ").withColumn(\n",
    "    \"is_hit_20\", col(\"hits_20\") > 0\n",
    ")\n",
    "\n",
    "# HitRate berechnen\n",
    "count_users = joined_data.count()\n",
    "# Berechne die durchschnittliche HitRate@10 und HitRate@20 über alle User\n",
    "hitrate_10 = hit_data.filter(col(\"is_hit_10\") == True).count() / count_users if count_users > 0 else None\n",
    "hitrate_20 = hit_data.filter(col(\"is_hit_20\") == True).count() / count_users if count_users > 0 else None\n",
    "\n",
    "print(f\"Datensatz 1 - HitRate@10: {hitrate_10}\")\n",
    "print(f\"Datensatz 1 - HitRate@20: {hitrate_20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6fa84-0eb3-4772-a374-299116660593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS-Algorithmus für Datensatz 2 (nach Angriff)\n",
    "als2 = ALS(\n",
    "    rank=100,\n",
    "    maxIter=20,\n",
    "    regParam=0.05,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"businessId_new\",\n",
    "    ratingCol=\"stars\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "als_model2 = als2.fit(train2)\n",
    "predictions2 = als_model2.transform(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfa4f2-422b-4038-9d7d-47393f26e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE und MAE für Datensatz 2 berechnen\n",
    "evaluator_rmse2 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"stars\", predictionCol=\"prediction\")\n",
    "evaluator_mae2 = RegressionEvaluator(metricName=\"mae\", labelCol=\"stars\", predictionCol=\"prediction\")\n",
    "rmse2 = evaluator_rmse2.evaluate(predictions2)\n",
    "mae2 = evaluator_mae2.evaluate(predictions2)\n",
    "\n",
    "print(f\"Datensatz 2 - RMSE: {rmse2}\")\n",
    "print(f\"Datensatz 2 - MAE: {mae2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5b300-8495-41d9-9924-11a9c97eb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HitRate-Berechnung für Datensatz 2\n",
    "\n",
    "# Anzahl der Partitionen optimieren\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "# Relevante Items nur einmal berechnen\n",
    "window_spec2 = Window.orderBy(col(\"stars\").desc())\n",
    "\n",
    "test2 = test2.withColumn(\"percentile\", percent_rank().over(window_spec2))\n",
    "# Verwende die besten 20% der Bewertungen als relevante Items und gruppiere nach userId\n",
    "relevant_items2 = test2.filter(col(\"percentile\") <= 0.2) \\\n",
    "                       .groupBy(\"userId\") \\\n",
    "                       .agg(expr(\"collect_list(businessId_new) as relevant_items\")) \\\n",
    "                       .cache()\n",
    "\n",
    "# Empfehlungen nur einmal berechnen\n",
    "top_k_recommendations = als_model2.recommendForAllUsers(20).cache()\n",
    "\n",
    "# Extrahiere empfohlene Items\n",
    "top_k_recommendations = top_k_recommendations.withColumn(\n",
    "    \"recommended_item_ids\",\n",
    "    expr(\"transform(recommendations, x -> x['businessId_new'])\")\n",
    ").select(\"userId\", \"recommended_item_ids\")\n",
    "\n",
    "# Verbinde mit relevanten Items\n",
    "joined_data = top_k_recommendations.join(broadcast(relevant_items2), on=\"userId\", how=\"inner\")\n",
    "\n",
    "# Berechne Treffer für k=10 und k=20 gleichzeitig\n",
    "hit_data = joined_data.withColumn(\n",
    "    \"hits_10\",\n",
    "    size(array_intersect(slice(col(\"recommended_item_ids\"), 1, 10), col(\"relevant_items\")))\n",
    ").withColumn(\n",
    "    \"hits_20\",\n",
    "    size(array_intersect(col(\"recommended_item_ids\"), col(\"relevant_items\")))\n",
    ").withColumn(\n",
    "    \"is_hit_10\", col(\"hits_10\") > 0\n",
    ").withColumn(\n",
    "    \"is_hit_20\", col(\"hits_20\") > 0\n",
    ")\n",
    "\n",
    "# Schritt 5: HitRate berechnen\n",
    "count_users = joined_data.count()\n",
    "# Berechne die durchschnittliche HitRate@10 und HitRate@20 über alle User\n",
    "hitrate_10 = hit_data.filter(col(\"is_hit_10\") == True).count() / count_users if count_users > 0 else None\n",
    "hitrate_20 = hit_data.filter(col(\"is_hit_20\") == True).count() / count_users if count_users > 0 else None\n",
    "\n",
    "print(f\"Datensatz 2 - HitRate@10: {hitrate_10}\")\n",
    "print(f\"Datensatz 2 - HitRate@20: {hitrate_20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94380a-d254-47ab-ab67-4ec24ce44978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne Vorhersagen auf den Trainingsdaten beider Modelle\n",
    "train_predictions1 = als_model1.transform(train1) \\\n",
    "    .select(\"userId\", \"businessId_new\", col(\"prediction\").alias(\"prediction_1\"))\n",
    "train_predictions2 = als_model2.transform(train2) \\\n",
    "    .select(\"userId\", \"businessId_new\", col(\"prediction\").alias(\"prediction_2\"))\n",
    "\n",
    "# Join über die gemeinsamen Schlüssel\n",
    "common_predictions = train_predictions1.join(train_predictions2, on=[\"userId\", \"businessId_new\"], how=\"inner\")\n",
    "\n",
    "# Berechne den absoluten Unterschied und den durchschnittlichen Prediction Shift\n",
    "common_predictions = common_predictions.withColumn(\"abs_diff\", abs(col(\"prediction_1\") - col(\"prediction_2\")))\n",
    "prediction_shift = common_predictions.agg({\"abs_diff\": \"avg\"}).collect()[0][0]\n",
    "print(f\"Prediction Shift: {prediction_shift}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2b4ca-7a3b-4cf9-ad28-406d94c78853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beende die Spark-Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
