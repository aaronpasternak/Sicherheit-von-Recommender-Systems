{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1436dc-e688-41c2-b40f-5dc44c7523cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der erforderlichen Bibliotheken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d91fb-9c61-4cd9-983a-0c30453ee4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benutzereingaben für Datensatz, Angriffstyp und Angriffsgröße\n",
    "datensatz = input(\"Bitte den Datensatznamen eingeben: \")\n",
    "angriffstyp = input(\"Bitte den Angriffstyp eingeben: \")\n",
    "angriffsgröße = input(\"Bitte die Angriffsgröße eingeben: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690b050-8282-4b1c-9259-1d0bc090b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen (CSV-Datei mit den entsprechenden Parametern)\n",
    "dateipfad = f\"ratings_readyforclassification_{datensatz}_{angriffstyp}_{angriffsgröße}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca54f04-2864-4f1d-a9ef-4b4823eaabaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Datensatz laden\n",
    "data = pd.read_csv(dateipfad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d85bd-9d91-464a-8eb8-1537fc322cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# userId Spalte entfernen, damit nur noch Label und die Features übrig bleiben\n",
    "data = data.drop(columns=['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b079c4-479f-42f4-af86-053dd3702493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige erste Einträge des Dataframes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abce517-07dc-47f8-a613-f12f37def96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige letzte Einträge des Dataframes\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64df68-3c53-4971-a928-038c028fda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merkmale und Zielvariable (Label) definieren\n",
    "X = data.drop(columns=[\"Label\"])  # Merkmale\n",
    "y = data[\"Label\"]  # Zielvariable (0 = normal, 1 = Angriff)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49811987-2598-4158-8a98-05bb435dca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalieren der Merkmale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9268e0d-a56b-4146-95d4-f5063d46db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold Setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7704fe-8da8-4967-9a29-66baf89eb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste für die Ergebnisse aller Modelle\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad286f-8049-40c4-a69b-da4ac5abe99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 1: Naive Bayes\n",
    "model_name = \"Naive Bayes\"\n",
    "model = GaussianNB()\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels\n",
    "y_pred_all = []  # Alle vorhergesagten Labels\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e2863-35bb-4be8-b3a6-742976c50204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 2: K-Nearest Neighbors\n",
    "model_name = \"K-Nearest Neighbors\"\n",
    "model = KNeighborsClassifier(metric='manhattan', n_neighbors=7, weights='distance')\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels (für Cross-Validation)\n",
    "y_pred_all = []  # Alle vorhergesagten Labels (für Cross-Validation)\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805eda5-8993-48be-847f-5a8afa960d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 3: Decision Tree\n",
    "model_name = \"Decision Tree\"\n",
    "model = DecisionTreeClassifier(max_depth=10, min_samples_leaf=4, min_samples_split=10, random_state=42)\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels (für Cross-Validation)\n",
    "y_pred_all = []  # Alle vorhergesagten Labels (für Cross-Validation)\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc2c59-04e3-4267-9d47-f8c3d192496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 4: Random Forest\n",
    "model_name = \"Random Forest\"\n",
    "model = RandomForestClassifier(n_estimators=200, min_samples_split=5, min_samples_leaf=1, max_depth=20, bootstrap=False, random_state=42)\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels (für Cross-Validation)\n",
    "y_pred_all = []  # Alle vorhergesagten Labels (für Cross-Validation)\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4b7aa-a3bf-4697-b18b-39de4a0f0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 5: Support Vector Machine\n",
    "model_name = \"Support Vector Machine\"\n",
    "model = SVC(kernel='rbf', gamma='scale', C=100, probability=True, random_state=42)\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels (für Cross-Validation)\n",
    "y_pred_all = []  # Alle vorhergesagten Labels (für Cross-Validation)\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.decision_function(X_test)  # decision_function für Scores\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd57c1e-635e-41ea-8991-d77e463d563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 6: Logistic Regression\n",
    "model_name = \"Logistic Regression\"\n",
    "model = LogisticRegression(C=10, penalty='l1', solver='liblinear', random_state=42)\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels (für Cross-Validation)\n",
    "y_pred_all = []  # Alle vorhergesagten Labels (für Cross-Validation)\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eebb41-c20a-42df-9c45-8d9ab7550956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 7: XGBoost\n",
    "model_name = \"XGBoost\"\n",
    "model = XGBClassifier(subsample=0.7, n_estimators=500, max_depth=5, learning_rate=0.1, \n",
    "                      colsample_bytree=0.8, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels (für Cross-Validation)\n",
    "y_pred_all = []  # Alle vorhergesagten Labels (für Cross-Validation)\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab0273-e641-4a1c-bb95-a11bec3c7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell 8: Stacking\n",
    "model_name = \"Stacking (Meta-Model)\"\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, min_samples_split=5, min_samples_leaf=1, max_depth=20, bootstrap=False, random_state=42)),\n",
    "        ('xgb', XGBClassifier(subsample=0.7, n_estimators=500, max_depth=5, learning_rate=0.1, colsample_bytree=0.8, use_label_encoder=False, eval_metric=\"logloss\", random_state=42))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(C=10, penalty='l1', solver='liblinear', random_state=42)\n",
    ")\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels (für Cross-Validation)\n",
    "y_pred_all = []  # Alle vorhergesagten Labels (für Cross-Validation)\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "    y_pred = stacking_model.predict(X_test)\n",
    "    y_pred_proba = stacking_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce115f-9277-40de-9ec8-cdb50714fd85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modell 9: Neural Network\n",
    "model_name = \"Neural Network\"\n",
    "model = MLPClassifier(activation='relu', hidden_layer_sizes=(50, 50), learning_rate='constant', \n",
    "                      solver='adam', random_state=42, max_iter=500)\n",
    "\n",
    "# Initialisierung der Variablen\n",
    "total_cm = np.zeros((2, 2))  # Gesamte Konfusionsmatrix\n",
    "accuracies = []\n",
    "pr_aucs = []  # PR-AUC Werte\n",
    "\n",
    "y_true_all = []  # Alle wahren Labels (für Cross-Validation)\n",
    "y_pred_all = []  # Alle vorhergesagten Labels (für Cross-Validation)\n",
    "\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Konfusionsmatrix und Metriken für diesen Fold\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    total_cm += cm\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # PR-AUC berechnen\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pr_aucs.append(pr_auc)\n",
    "\n",
    "    # Wahre und vorhergesagte Labels für den Classification Report sammeln\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Classification Report auf Basis der Cross-Validation-Vorhersagen\n",
    "print(f\"Classification Report für {model_name} (Cross-Validation):\\n\")\n",
    "report = classification_report(y_true_all, y_pred_all, target_names=[\"Normal (0)\", \"Angriff (1)\"], digits=6, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "# Durchschnittswerte über alle Folds berechnen\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_pr_auc = np.mean(pr_aucs)\n",
    "\n",
    "print(f\"Durchschnittliche Accuracy (Cross-Validation): {average_accuracy:.6f}\")\n",
    "print(f\"Durchschnittliche PR-AUC (Cross-Validation): {average_pr_auc:.6f}\")\n",
    "\n",
    "# Durchschnittliche Konfusionsmatrix\n",
    "average_cm = total_cm / skf.n_splits\n",
    "average_cm_rounded = np.round(average_cm).astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=average_cm_rounded, display_labels=[\"Normal (0)\", \"Angriff (1)\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Durchschnittliche Confusion Matrix für {model_name}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall-Kurve für den letzten Fold plotten\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.6f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Kurve für {model_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Ergebnisse speichern (für Cross-Validation)\n",
    "results.append({\n",
    "    \"Algorithm\": model_name,\n",
    "    \"Precision (Klasse 1)\": report[\"Angriff (1)\"][\"precision\"],\n",
    "    \"Recall (Klasse 1)\": report[\"Angriff (1)\"][\"recall\"],\n",
    "    \"F1-Score (Klasse 1)\": report[\"Angriff (1)\"][\"f1-score\"],\n",
    "    \"PR-AUC\": average_pr_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66f5be-09e5-4598-85e7-c7a1b1cc3478",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Tabelle mit Ergebnissen erstellen\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb30ab-508a-49b1-9436-d2b984272413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Spalte \"Precision (Klasse 1)\" (gerundet auf 6 Nachkommastellen)\n",
    "print(\"\\nPrecision (Klasse 1):\")\n",
    "print(\"\\n\".join(results_df['Precision (Klasse 1)'].round(6).astype(str)))\n",
    "\n",
    "# Ausgabe der Spalte \"Recall (Klasse 1)\" (gerundet auf 6 Nachkommastellen)\n",
    "print(\"\\nRecall (Klasse 1):\")\n",
    "print(\"\\n\".join(results_df['Recall (Klasse 1)'].round(6).astype(str)))\n",
    "\n",
    "# Ausgabe der Spalte \"F1-Score (Klasse 1)\" (gerundet auf 6 Nachkommastellen)\n",
    "print(\"\\nF1-Score (Klasse 1):\")\n",
    "print(\"\\n\".join(results_df['F1-Score (Klasse 1)'].round(6).astype(str)))\n",
    "\n",
    "# Ausgabe der Spalte \"PR-AUC\" (gerundet auf 6 Nachkommastellen)\n",
    "print(\"\\nPR-AUC:\")\n",
    "print(\"\\n\".join(results_df['PR-AUC'].round(6).astype(str)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
